# -*- coding: utf-8 -*-
"""FashionRecommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ldcYO4i_uhkXxsQoztTL17MUHJzs86OT
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
paramaggarwal_fashion_product_images_dataset_path = kagglehub.dataset_download('paramaggarwal/fashion-product-images-small')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

#import numpy as np # linear algebra
#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

#import os
#for dirname, _, filenames in os.walk(paramaggarwal_fashion_product_images_dataset_path):
#    for filename in filenames:
#        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import GlobalMaxPooling2D
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import numpy as np
from numpy.linalg import norm
import os
from tqdm import tqdm
import pandas as pd
import pickle

model=ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))
model.trainable=False
model=tf.keras.Sequential([
    model,
    GlobalMaxPooling2D()
])
print(model.summary())

def extract_features(img_path,model):
  img=image.load_img(img_path,target_size=(224,224))
  img_array=image.img_to_array(img)
  expanded_img_array=np.expand_dims(img_array,axis=0)
  preprocessed_img=preprocess_input(expanded_img_array)
  result=model.predict(preprocessed_img).flatten()
  normalized_result=result/norm(result)
  return normalized_result

images_list=[]
for dirname, _, filenames in os.walk(paramaggarwal_fashion_product_images_dataset_path):
    if(len(filenames)==44441):
      for filename in filenames:
        images_list.append(os.path.join(dirname, filename))
      break

feature_list=[]
for i in tqdm(images_list[0:100]):  #generating embedding for just 100 due to lack of hardware resources for proper training
  feature_list.append(extract_features(i,model))

pickle.dump(images_list,open('images.pkl','wb'))
pickle.dump(feature_list,open('features.pkl','wb'))

from sklearn.neighbors import NearestNeighbors
neighbors=NearestNeighbors(n_neighbors=6,algorithm='brute',metric='euclidean')
neighbors.fit(feature_list)

curr_img_features=extract_features(images_list[0],model)

distances,indices = neighbors.kneighbors([curr_img_features])
print(indices)

import cv2
from google.colab.patches import cv2_imshow
for i in indices[0][1:5]:
  temp_img = cv2.imread(images_list[i])
  cv2_imshow(cv2.resize(temp_img,(512,512)))
  cv2.waitKey(0)

ka